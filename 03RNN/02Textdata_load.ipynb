{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text\n",
    "\n",
    "서로 다른 작가가 호머의 일리아드를 영어로 번역한 3개의 텍스트를 읽어 들여 적절하게 데이터 전처리를 수행하고 데이터 셋으로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text 파일 준비하기\n",
    "\n",
    "호머의 일리아드를 영어로 번역한 3개의 텍스트를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aoneko\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "DIRECTORY_URL = os.environ['USERPROFILE'] + '\\\\.keras\\\\datasets'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3개의 텍스트 파일을 읽어들여 하나의 데이터셋으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"癤풞chilles sing, O Goddess! Peleus' son;\\n\", 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []     # 텍스트\n",
    "labels = []   # labels 0=cowper, 1=derby, 2=butler\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    with open(os.path.join(parent_dir, file_name)) as f:\n",
    "        lines = f.readlines()\n",
    "        cls = [i] * len(lines)\n",
    "        data.extend(lines) \n",
    "        labels.extend(cls)\n",
    "\n",
    "data[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "* Tokenizer API를 사용하여 해당 문서를 token으로 분리한다.\n",
    "* texts_to_sequences() 로 해당 단어를 숫자로 encode한다.\n",
    "* pad_sequences()를 사용하면 해당 단어의 sequence의 길이를 모두 동일하게 맞출 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = Tokenizer()\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10414,  5198,   381,   257,   429,    21,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(data)\n",
    "\n",
    "tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`index_word()`  \n",
    "tokenize 결과를 다시 단어 시퀀스로 변환하는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10414 ----> 癤풞chilles\n",
      "5198 ----> sing\n",
      "381 ----> o\n",
      "257 ----> goddess\n",
      "429 ----> peleus'\n",
      "21 ----> son\n"
     ]
    }
   ],
   "source": [
    "convert(tokenizer, tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tf.data 데이터 세트 만들기\n",
    "\n",
    "딥러닝 모형에 입력으로 사용할 수 있게 텍스트 데이터를 데이터셋으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15537\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(tensor)\n",
    "BATCH_SIZE = 64\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((tensor, labels)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 19), dtype=int32, numpy=\n",
       " array([[  12,  368,  116, ...,    0,    0,    0],\n",
       "        [  14,   39,  267, ...,    0,    0,    0],\n",
       "        [  36,  100,   20, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1040,    8, 3912, ...,    0,    0,    0],\n",
       "        [   5,  799, 1365, ...,    0,    0,    0],\n",
       "        [   5, 1303,  908, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
       " array([0, 2, 2, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 1, 2, 2, 1,\n",
       "        0, 2, 1, 2, 2, 0, 2, 1, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 2, 0, 1, 1,\n",
       "        2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1])>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
