{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bQhc2-CA8aJ"
   },
   "source": [
    "# 텍스트 데이터 전처리 하기\n",
    "\n",
    "딥러닝 모형에서는 텍스트를 수치로 변환하여 처리해야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWB5zPqrsnhS"
   },
   "source": [
    "# BoW(Bag of Word)\n",
    "\n",
    "텍스트나 단어를 사용하기 전에 수치 형태로 변환하는 전처리 과정을 거쳐야 한다. 텍스트를 수치 벡터로 표현하는 방법으로 BoW가 있다.\n",
    "\n",
    "1. 전체 문서를 고유한 token으로 변환한다.\n",
    "2. 특정 문서에서 각 단어가 얼마나 자주 등장하는지 세어서 문서의 특성 벡터를 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxt7hMjeTeqw"
   },
   "source": [
    "### CountVectorizer\n",
    "\n",
    "CountVectorizer는 이러한 작업을 하기 위한 다음과 같은 인수를 가질 수 있다.\n",
    "\n",
    "* stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "* stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "* analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "* token_pattern : string\n",
    "토큰 정의용 정규 표현식\n",
    "* tokenizer : 함수 또는 None (디폴트)\n",
    "토큰 생성 함수 .\n",
    "* ngram_range : (min_n, max_n) 튜플\n",
    "n-그램 범위\n",
    "* max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최대 빈도\n",
    "* min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최소 빈도\n",
    "\n",
    "어휘 사전의 내용을 출력해 보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1573917574861,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "9c0vgfOVTLHi",
    "outputId": "4b05fc87-a206-4b47-e321-bb7f420a86bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2255,
     "status": "ok",
     "timestamp": 1573917574862,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "cLaqHGePTQPM",
    "outputId": "1249fb17-f3b6-4e71-e2c6-b319b05907e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUfhGAcLTTxM"
   },
   "source": [
    "# Load Text\n",
    "\n",
    "RNN에서 텍스트 데이터 전처리는 BoW와는 달리 고유한 단어의 집합만 관심 대상이고 부수적으로 생성된 단어의 빈도는 필요하지 않다.\n",
    "\n",
    "tf.data.TextLineDataset를 사용하면 텍스트 파일로 부터 데이터 셋을 생성할 수 있다. \n",
    "\n",
    "서로 다른 작가가 호머의 일리아드를 영어로 번역한 3개의 텍스트를 읽어 들여 적절하게 데이터 전처리를 수행하고 데이터 셋으로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4874,
     "status": "ok",
     "timestamp": 1573917577494,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "WUPqwqYSpB_O",
    "outputId": "07953df2-d8ac-4401-9fae-72fe30eaf27d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4862,
     "status": "ok",
     "timestamp": 1573917577494,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "A874OijIp5XU",
    "outputId": "9f8f2de5-072c-4709-d261-8f5e1cdbca9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chong\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "DIRECTORY_URL = os.environ['USERPROFILE'] + '\\\\.keras\\\\datasets'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ag8gKop7p-_k"
   },
   "source": [
    "### Load text into datasets\n",
    "\n",
    "tf.data.Dataset.map을 사용하여 labeler function을 적용하면 데이터 셋의 각 데이터에 대해서 (example, label) 쌍으로 리턴해 준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vx82Ovoqjiw"
   },
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEDWodgEqtuo"
   },
   "source": [
    "각 각의 labeled 데이터 셋을 하나의 데이터 셋으로 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGjlR6gUql4v"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_8cf3LQq0NR"
   },
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "    all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GX4hM9Eq9FK"
   },
   "source": [
    "tf.data.Dataset.take 를 사용하여 데이터 셋에서 데이터를 읽어 들여 화면에 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6124,
     "status": "ok",
     "timestamp": 1573917578816,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "3nACgbLLrBME",
    "outputId": "6396e59f-e4cc-4712-c410-f1cad497a217",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Then the princes of the Achaeans took the son of Peleus to Agamemnon,'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'mankind. They forget the promise they made you when they set out from'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'killed him. There must be a god who is angry with me. Moreover I have'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"same father, went also, with Pandion to carry Teucer's bow. They went\">, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Rare acquisition! and at every board'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyGwLl8FrXuo"
   },
   "source": [
    "# Build vocabulary\n",
    "\n",
    "tfds.features.text.Tokenizer를 사용하여 각 데이터를 토큰화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11312,
     "status": "ok",
     "timestamp": 1573917584016,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "4yTu3J74r92b",
    "outputId": "c79407dc-f0e7-436d-adbe-df2b29c66dfb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1f226e820847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvocabulary_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_labeled_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msome_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atS2liWosEjK"
   },
   "source": [
    "### Encode examples\n",
    "encode method 는 문자열을 입력으로 받아 integer 리스트를 리턴해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHBRo1zdsQHe"
   },
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12792,
     "status": "ok",
     "timestamp": 1573917585574,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "iXnXUdHQsTAY",
    "outputId": "af2bde61-076a-4ef4-dc4b-60633608fc61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Whom godlike Paris had from Sidon brought,'\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12784,
     "status": "ok",
     "timestamp": 1573917585575,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "eu47MWqmsV_o",
    "outputId": "cd6a4f0b-a9e0-496f-ddad-a734c68f10f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9922, 7231, 10378, 11522, 12215, 12483, 14814, 695, 10032, 11669, 2677, 695, 3664]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CgRYIe0sa5g"
   },
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "\n",
    "# tf.py_function을 사용하여 tensorflow에서 파이썬 함수를 호출한다.\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14144,
     "status": "ok",
     "timestamp": 1573917586948,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "OMhfox6Ut97U",
    "outputId": "cf71177b-c506-4885-bc01-9a1f1a551347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: id=99331, shape=(13,), dtype=int64, numpy=\n",
       "  array([ 9922,  7231, 10378, 11522, 12215, 12483, 14814,   695, 10032,\n",
       "         11669,  2677,   695,  3664])>,\n",
       "  <tf.Tensor: id=99332, shape=(), dtype=int64, numpy=2>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_encoded_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7BkcuQ-wU1z"
   },
   "source": [
    " 각 텍스트 줄의 단어 수는 다르다. 따라서 tf.data.Dataset.padded_batch을 사용 하여 예제를 동일한 크기로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiaWNGsYvcZ4"
   },
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29834,
     "status": "ok",
     "timestamp": 1573917602657,
     "user": {
      "displayName": "이선화",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAaVHDHE1MIY1LHmJ_yWOBQMBjk_0EAJ4f0eWo-=s64",
      "userId": "17814374716323603424"
     },
     "user_tz": -540
    },
    "id": "Qma_X6Vmud4R",
    "outputId": "9ee7fcf8-396d-4548-e958-c65942f81b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=248191, shape=(15,), dtype=int64, numpy=\n",
       " array([11237,  5190,  1075,  2118,  9523, 10869,  5562,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])>,\n",
       " <tf.Tensor: id=248195, shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(train_data))\n",
    "\n",
    "print(sample_text.shape, sample_labels.shape)\n",
    "\n",
    "sample_text[0], sample_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "텍스트전처리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
