{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 데이터 전처리\n",
    "\n",
    "딥러닝을 위한 텍스트 데이터를 준비하는 방법\n",
    "\n",
    "딥러닝 모형에서는 텍스트를 수치로 변환하여 처리해야 한다.\n",
    "\n",
    "원시 텍스트(raw text)를 딥러닝 모델에 직접 공급할 수 없다.\n",
    "\n",
    "텍스트 데이터는 기계 학습 및 심층 학습 모델의 입력 또는 출력으로 사용할 숫자로 인코딩되어야 한다.\n",
    "\n",
    "\n",
    "* 텍스트 데이터를 빠르게 준비하는 데 사용할 수있는 편리한 방법.\n",
    "    * BoW(Bag of Word)\n",
    "    * Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW(Bag of Word)\n",
    "\n",
    "텍스트나 단어를 사용하기 전에 수치 형태로 변환하는 전처리 과정을 거쳐야 한다. 텍스트를 수치 벡터로 표현하는 방법으로 BoW가 있다.\n",
    "\n",
    "1. 전체 문서를 고유한 token으로 변환한다.\n",
    "2. 특정 문서에서 각 단어가 얼마나 자주 등장하는지 세어서 문서의 특성 벡터를 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "CountVectorizer는 이러한 작업을 하기 위한 다음과 같은 인수를 가질 수 있다.\n",
    "\n",
    "* stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "* stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "* analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "* token_pattern : string\n",
    "토큰 정의용 정규 표현식\n",
    "* tokenizer : 함수 또는 None (디폴트)\n",
    "토큰 생성 함수 .\n",
    "* ngram_range : (min_n, max_n) 튜플\n",
    "n-그램 범위\n",
    "* max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최대 빈도\n",
    "* min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최소 빈도\n",
    "\n",
    "어휘 사전의 내용을 출력해 보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)\n",
    "\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN을 위한 텍스트 데이터 전처리\n",
    "\n",
    "RNN에서 텍스트 데이터 전처리는 BoW와는 달리 고유한 단어의 집합만 관심 대상이고 부수적으로 생성된 단어의 빈도는 필요하지 않다.\n",
    "\n",
    "\n",
    "* text_to_word_sequence로 단어를 분할한다.\n",
    "* Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_to_word_sequence로 단어 분할\n",
    "\n",
    "텍스트로 작업 할 때 첫 번째 단계는 텍스트를 단어로 나누는 것이다.  \n",
    "단어를 토큰(Token)이라고하며 텍스트를 토큰으로 분할하는 과정을 토큰 화(Tokenizer)라고 한다.\n",
    "\n",
    "` text_to_word_sequence () 함수 `는 다음의 작업을 자동으로 수행한다.\n",
    "\n",
    "* 공백으로 단어를 분할합니다 (split =”“).\n",
    "* 구두점을 필터링합니다 (filters = '!”# $ % & () * +,-. / :; <=>? @ [\\\\] ^ _`{|} ~ \\ t \\ n').\n",
    "* 텍스트를 소문자로 변환합니다 (lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer API\n",
    "여러 텍스트 문서를 준비하는 데 적합하고 재사용 할 수있는 텍스트를 준비하기위한보다 정교한 API를 제공한다.\n",
    " \n",
    "`Tokenizer 클래스`  참고) https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer  \n",
    "Tokenizer는 문서에 대해 학습 한 내용을 쿼리하는 데 사용할 수있는 4 가지 속성을 제공한다.\n",
    "\n",
    "* word_counts : 단어 사전 및 개수.\n",
    "* word_docs : 단어 사전 및 각 단어가 등장하는 문서 수.\n",
    "* word_index : 단어 사전과 고유하게 할당 된 정수입니다.\n",
    "* document_count : Tokenizer에 맞게 사용 된 총 문서 수의 정수 수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Tokenizer()` not found.\n"
     ]
    }
   ],
   "source": [
    "?Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texts_to_matrix ()` 메소드  \n",
    "\n",
    "* Tokenizer의 `texts_to_matrix () 함수`를 사용하여 입력 제공되는 문서 당 하나의 벡터를 만들 수 있다. \n",
    "* 벡터의 길이는 어휘의 총 크기이다.\n",
    "* 표준 bag-of-words 모델 텍스트 인코딩 체계 모음을 제공\n",
    "    - ' binary ': 문서에 각 단어가 있는지 여부. 이것이 기본값입니다.\n",
    "    - ' count ': 문서에있는 각 단어의 개수입니다.\n",
    "    - ' tfidf ': 문서의 각 단어에 대한 텍스트 빈도 역 문서 빈도 (TF-IDF) 점수입니다.\n",
    "    - ' freq ': 각 문서 내에서 단어의 비율로서 각 단어의 빈도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texts_to_sequences()` 메소드\n",
    "\n",
    "* Tokenizer의 texts_to_sequences()를 사용하면 각 문서의 텍스트를 정수 시퀀스로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 3], [4, 1], [5, 6], [7, 1], [8]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs)\n",
    "t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
