{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 계산 그래프의 이해\n",
    "\n",
    "계산 그래프를 만든 후 세션을 통해 그래프를 실행한다.\n",
    "1. 비어있는 계산그래프 생성\n",
    "2. 계산 그래프에 노드(텐서와 연산)을 추가\n",
    "3. 그래프 실행\n",
    "    * 새로운 새션을 시작\n",
    "    * 그래프에 있는 변수를 초기화\n",
    "    * 계산 그래프를 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__ # 텐서플로우 버전확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2e7b16bba66c429696bf3a443cc6ea1e"
   },
   "source": [
    "### Variable, 변수형 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "cdb9d4e7fffa4169a6d158abc0bdee42"
   },
   "source": [
    "변수형 텐서는 텐서의 값이 바뀔 수 있다. `Variable` 클래스로 정의하며 항상 초기값을 지정해 주어야 한다. 자료형과 크기는 초기값으로부터 자동으로 유추한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "school_cell_uuid": "e990ff5d2f59440c94ae9f7b0991d40f"
   },
   "outputs": [],
   "source": [
    "# 실수 변수형 텐서\n",
    "s = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "school_cell_uuid": "700555191d8c4c5ea60e93dd20bbd434"
   },
   "outputs": [],
   "source": [
    "# 벡터 변수형 텐서\n",
    "v = tf.Variable(tf.ones((2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "school_cell_uuid": "bfc99efe9c0f4148a8aaa14f773e92d1"
   },
   "outputs": [],
   "source": [
    "# 행렬 변수형 텐서\n",
    "x = tf.Variable(tf.ones((2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "91d7f5bc02ed4155b989fdcc615933c2"
   },
   "source": [
    "변수 텐서의 값을 바꿀 때는 `assign`, `assign_add`, `assign_sub` 메서드를 사용한다.\n",
    "\n",
    "* `assign`: 값을 완전히 할당. `=`에 해당\n",
    "* `assign_add`: 값을 증가. `+=`에 해당\n",
    "* `assign_sub`: 값을 감소. `-=`에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "school_cell_uuid": "379d34438d2d4c4897e940c91379d348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다음과 같이 하면 안된다!. 변수헝 텐서가 상수형 텐서로 변한다!\n",
    "# x = tf.ones((2, 1))\n",
    "x.assign(tf.ones((2, 1)))\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Execution\n",
    "TensorFlow v1의 경우 그래프 생성과 그래프 실행을 분리하고, Lazy Evaluation 형태로 세션을 열고 그래프를 실행하는 시점에서 실제값이 계산되는 구조였다. 이는 성능을 위한 선택이었지만 이로 인해 디버깅이 불편하고 직관적인 형태로 프로그래밍이 불가능하다는 점이 계속해서 문제점으로 지적되었다. 따라서 TensorFlow v2에서는 별도의 세션을 통한 실행없이 바로 그래프의 특정값을 계산할수 있는 Eager Execution을 기본적으로 적용하였다. 따라서 모든 코드는 쓰여진 라인 순서에 따라 실행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "619977e7107c46fbbb9c2d95fd8da776"
   },
   "source": [
    "### Tensorflow function 와 Graph\n",
    "\n",
    "Tensorflow 1에서 graph는 tensorflow API의 핵심이므로 피할 수가 없었다. 이 때문에 복잡도가 높아졌다. tensorflow 2에서도 graph가 있지만 이전만큼 핵심적이지는 않고 사용하기 쉬워졌다. \n",
    "\n",
    "\n",
    "Tensorflow function은 파이토치(PyTorch) 등의 다른 딥러닝 라이브러리에 있는 함수 기능을 본따 텐서플로우 버전 2.0 에서 새로 만들어진 방법이다. 함수를 사용하면 텐서플로우 버전 1에서처럼 플레이스홀더(placeholder)와 계산 그래프 등을 명시적으로 사용하지 않고 선언적으로 계산 과정을 구현할 수 있다.\n",
    "\n",
    "\n",
    "간단한 예로 세 제곱을 계산하는 함수를 만들어 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "school_cell_uuid": "2232e9e80fa741718daa1c2d45850d1c"
   },
   "outputs": [],
   "source": [
    "# 일반 파이썬 함수 정의\n",
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor를 사용하여 함수를 호출할 수 있다.\n",
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @tf.function 데코레이터\n",
    "\n",
    "함수에서 수행되는 계산을 분석하고 동일한 작업을 수행하는 계산그래프(computational graph)를 생성한다.\n",
    "\n",
    "텐서플로는 해당 함수를 최적화하여 계산 그래프를 생성한다. 최적화된 그래프가 준비되면 텐서플로우 함수는 적당한 순서로 그래프 내의 연산을 효율적으로 실행한다. 일반적으로 텐서플로 함수는 원본 파이썬 함수보다 훨씬 빠르게 실행된다. 특히 복잡한 연산 수행에 더 두드러진다. \n",
    "\n",
    "\n",
    "사용자 정의 함수를 작성하고 이를 케라스 모형에서 사용할 때는 케라스가 이 함수를 자동으로 텐서플로 함수로 변환한다. 따라서 tf.function을 사용할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x) = Wx + b $ 를 구하는 tensorflow 함수를 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "b\n",
      " [0. 0.]\n",
      "\n",
      "out_a\n",
      " tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "print(\"W\\n\", W.numpy())\n",
    "print(\"\\nb\\n\", b.numpy())\n",
    "\n",
    "@tf.function\n",
    "def forward(x):\n",
    "    return W * x + b\n",
    "\n",
    "out_a = forward([1,0])\n",
    "print(\"\\nout_a\\n\",out_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동 미분, Automatic Differentiation\n",
    "\n",
    "텐서플로우에서는 backpropagation에서 gradient 를 계산하기 위해 자동 미분 기능을 제공한다.\n",
    "TensorFlow에서 자동 미분 기능은 `tf.GradientTape`를 통해 수행된다. 자동 미분 기능은 간단히“autodiff”라고도 한다. \n",
    "\n",
    "`tf.GradientTape`는 tensor에 행해지는 각종 operation(연산)을 tracking(추적) 한다. 이러한 tracking을  tensor가 \"watched\"된다고 표현하기도 한다. 기본적으로 `tf.GradientTape`는 모형에서 훈련되는 변수들(예를 들면 가중치)를 자동으로 \"watch\"한다. 훈련변수는 `trainable=True` 로 설정하는데 tf.keras 에서 훈련되는 변수들은 `trainable = True` 로 초기화가 이루어 진다. \n",
    "\n",
    "일반적인 constant tensor를 수동으로 \"watched\" 상태로 하기 위해서 명시적으로 watch method를 호출 할 수 있다.\n",
    "\n",
    "\n",
    "다음과 같은 간단한 예를 보자. :\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "위 식의 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{d y}{d x} = 2x\n",
    "$$\n",
    "\n",
    " `tf.GradientTape`로 미분을 구하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient calculated by tf.GradientTape:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "True Gradient:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "Maximum Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# Print the gradient calculated by tf.GradientTape\n",
    "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
    "\n",
    "# Print the actual gradient of y = x^2\n",
    "print('\\nTrue Gradient:\\n', true_grad)\n",
    "\n",
    "# Print the maximum difference between true and calculated gradient\n",
    "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c55913d8a2e3415392d8b433cc79bade"
   },
   "source": [
    "### 자동 미분,  Auto diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5e131ec7447344cd840153db1c91c47f"
   },
   "source": [
    "변수 텐서 혹은 변수 텐서를 포함하는 연산의 결과로 만들어진 텐서를 입력으로 가지는 함수는 그 변수 텐서로 미분한 값을 계산할 수 있다. \n",
    "\n",
    "1. `GradientTape()`로 만들어지는 gradient tape 컨텍스트 내에서 함수값 결과를 저장한 텐서 `y`를 만든다.\n",
    "2. `tape.gradient(y, x)` 명령으로 변수형 텐서 `x`에 대한 `y`의 미분값을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "school_cell_uuid": "aa1fc667660d473eadde087d08ca8ae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.constant(1.0))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.multiply(5, x)\n",
    "\n",
    "gradient = tape.gradient(y, x) \n",
    "gradient.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "070063682d9e41abba7fd3bbee7b6e38"
   },
   "source": [
    "동시에 여러 변수에 대한 그레디언트 벡터를 구할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "school_cell_uuid": "eba29f88f2c445a788698dc003125f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(tf.constant(1.0))\n",
    "x2 = tf.Variable(tf.constant(1.0))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.multiply(x1, x2)\n",
    "\n",
    "gradients = tape.gradient(y, [x1, x2]) \n",
    "gradients[0].numpy(), gradients[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e36b23da1c9643db8d3cfe60ef5eab84"
   },
   "source": [
    "이 때 미분하는 텐서가 변수가 아니라 상수형이면 결과로는 `None`이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "school_cell_uuid": "eae0c4f7d9be4853b5a29441c4f226dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(tf.constant(1.0))\n",
    "a = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.multiply(a, x)\n",
    "\n",
    "gradient = tape.gradient(y, a) \n",
    "gradient is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "0f65279c2fd1474bbd7c15637be112a8"
   },
   "source": [
    "만약 상수형 텐서에 대해 미분하고 싶으면 `tape.watch()` 함수를 사용하여 상수형 텐서를 변수형 텐서처럼 바꿔야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "school_cell_uuid": "646db75fccce4caf89b4e4d1a9124863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    y = tf.multiply(a, x)\n",
    "\n",
    "gradient = tape.gradient(y, a) \n",
    "gradient.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
